# **HCI-IC Paper 2: Measuring Interaction Coherence — Formal Parameters and Instrumentation**

**Author:** James Thomas Hebert II  
**Institution:** Caelusyn Research Cooperative

---

## **Abstract**

This paper operationalizes the theoretical constructs of HCI-IC into measurable parameters. We formalize **drift magnitude (Δ)**, **coherence window duration (W_c)**, **interaction bandwidth (B)**, and **compression–expansion ratio (R_ce)** as functions of interaction state. Instrumentation for real‑time tracking is implemented within the Caelusyn Research Platform. Validation across varied task domains confirms that interaction coherence is quantifiable independent of task success or model performance, establishing a foundation for applied coherence tuning and operator training. All parameters are derived from observable interaction data without requiring access to model internals, ensuring platform-agnostic theoretical validity while demonstrating practical measurement through instrumented implementation.

---

## **1. From Constructs to Parameters**

Paper 1 defined five core constructs: Structured Vector Directive Protocol (SVDP), Flow‑Break Protocol (FBP), High‑Bandwidth Task Chaining (HBTC), the coherence envelope, and the Interaction Coherence Loop (ICL). Those constructs describe *what* stabilizes interaction; this paper defines *how* to measure that stability.

Measurement must occur at the **interaction level**, not the turn level. A single turn can be perfectly formed yet still belong to a drifting exchange; conversely, a locally ambiguous turn may not break a coherent window if the surrounding vectors remain aligned. Therefore, every parameter defined here is a **relational property of the interaction stream**, not an attribute of isolated prompts or responses.

### **1.1 Guiding Principles**

Three principles guide the parameterization:

1. **Observable:** Each parameter must be derivable from logged interaction data (prompts, responses, timestamps, constraint markers) without requiring internal model states or subjective interpretation. All measurements rely solely on:
   - Turn text content (prompts and responses)
   - Explicit structural markers (SVDP headers, FBP signals, constraint declarations)
   - Temporal data (timestamps, turn indices)
   - Session state (anchor stack, active constraints, window boundaries)

2. **Non‑reductive:** Parameters should capture the **dynamics** of interaction (trends, rates, durations) rather than reducing coherence to a static score. We measure:
   - Rates of change (drift velocity, bandwidth fluctuation)
   - Duration of states (window longevity, stability periods)
   - Structural rhythms (compression-expansion patterns)
   - Trajectory characteristics (recovery speeds, collapse predictors)

3. **Platform‑agnostic in theory, instrumented in practice:** The definitions are general and could be applied to any LLM interaction system. However, practical measurement requires instrumentation—session‑state tracking, intent anchoring, and real‑time logging—as implemented in the Caelusyn Platform. This instrumentation serves as both proof-of-concept and reference implementation.

### **1.2 Measurement Philosophy**

Traditional LLM evaluation focuses on outputs: accuracy, fluency, factuality, safety. These metrics assess *what* the model produces. Coherence parameters instead assess *how* the interaction unfolds—the communicative process itself. A perfectly accurate response can still belong to a drifting interaction if it misaligns with the user's evolving intent vector. Conversely, a technically flawed response might maintain coherence if it remains structurally aligned with established constraints and enables productive continuation.

This distinction is critical. Coherence measurement is not a replacement for output quality metrics; it is an orthogonal dimension. High coherence without output quality yields efficient production of useless results. High output quality without coherence yields correct answers that don't compose into larger workflows. Both dimensions are necessary; neither is sufficient.

### **1.3 The Interaction State Space**

Every interaction occupies a point in a high-dimensional state space defined by:
- **Intent vector (I):** The current anchored goal, derived from the most recent SVDP directive and all active constraints.
- **Context accumulation (C):** The information content successfully integrated from prior turns (definitions, partial results, established facts).
- **Constraint set (K):** The active boundaries on output form and content (format specifications, scope limits, mandatory inclusions/exclusions).
- **Exploration state (E):** The balance between compression (consolidation) and expansion (exploration) in recent turns.

Parameters Δ, B, W_c, and R_ce are functions that map trajectories through this state space to scalar or temporal measurements. They quantify different aspects of how the interaction point moves:
- **Δ** measures distance from the anchored intent vector I
- **W_c** measures duration of bounded excursions in I-space
- **B** measures effective velocity through C-space (information accumulation rate)
- **R_ce** measures oscillation pattern in E-space (structural rhythm)

This paper presents the formal definitions, the instrumentation that realizes them, and validation data demonstrating their stability and predictive power.

---

## **2. Drift Magnitude (Δ)**

**Definition.** Drift magnitude, denoted Δ, is the rate of misalignment accumulation per interaction turn. It quantifies how quickly the exchange deviates from the anchored intent vector established at the start of a coherence window.

### **2.1 Formal Definition**

Let \( I_0 \) represent the anchored intent vector at the start of a coherence window, established via explicit SVDP directive. Let \( S_t \) represent the interaction state at turn \( t \), composed of:
- The semantic content of the current exchange
- The active constraint set
- The accumulated context

Drift magnitude at turn \( t \) is:

\[
\Delta_t = \alpha \cdot d(S_t, I_0) + \beta \cdot V_t + \gamma \cdot R_t
\]

Where:
- \( d(S_t, I_0) \) is the semantic deviation (detailed below)
- \( V_t \) is the constraint violation count
- \( R_t \) is the recovery-attempt frequency
- \( \alpha, \beta, \gamma \) are weighting coefficients (empirically derived)

The window-level drift is the exponentially-weighted moving average:

\[
\Delta_{window} = \sum_{i=0}^{n} w_i \cdot \Delta_{t-i}
\]

with \( w_i = \frac{e^{-i/\tau}}{\sum_j e^{-j/\tau}} \) and time constant \( \tau = 3 \) turns.

### **2.2 Semantic Deviation Measurement**

**Embedding Model.** We use all-MiniLM-L6-v2 (384-dimensional sentence embeddings) for all semantic distance computations. This model was selected for:
- Low computational overhead (suitable for real-time use)
- Strong performance on semantic similarity tasks
- Deterministic behavior (no stochastic sampling)

**Distance Metric.** We compute cosine distance:

\[
d(S_t, I_0) = 1 - \frac{\mathbf{v}_{S_t} \cdot \mathbf{v}_{I_0}}{|\mathbf{v}_{S_t}| \cdot |\mathbf{v}_{I_0}|}
\]

Cosine distance (range [0, 2]) was chosen over Euclidean distance because:
- It is magnitude-invariant (verbose vs. terse turns don't inflate distance)
- It captures semantic orientation independent of embedding scale
- It exhibits more stable behavior across task domains in preliminary testing

**State Representation.** The state vector \( \mathbf{v}_{S_t} \) is constructed as:

\[
\mathbf{v}_{S_t} = \text{embed}(\text{concat}(P_t, R_{t-1}, K_t))
\]

Where:
- \( P_t \) is the current user prompt
- \( R_{t-1} \) is the previous assistant response (or empty for \( t=0 \))
- \( K_t \) is the serialized constraint set (format: "Constraint: value; ...")

This concatenation captures the full communicative context at turn \( t \). The intent vector \( \mathbf{v}_{I_0} \) uses the same construction at the window-start turn.

**Normalization.** Raw cosine distance is normalized by task complexity:

\[
d_{norm}(S_t, I_0) = \frac{d(S_t, I_0)}{1 + \log(1 + T_{complexity})}
\]

Where \( T_{complexity} \) is the task complexity score (range [1, 10]) assigned per task type:

| Task Type | Complexity Score | Rationale |
|-----------|-----------------|-----------|
| Simple retrieval | 1 | Single fact lookup, minimal context |
| Algorithm implementation | 7 | Multi-step reasoning, constraint management |
| Research synthesis | 8 | Source integration, contradiction resolution |
| UI specification | 6 | Hierarchical structure, cross-referential constraints |
| Project planning | 5 | Decomposition, prioritization, dependency tracking |

This logarithmic scaling prevents high-complexity tasks from dominating the drift metric solely due to their inherent semantic breadth.

### **2.3 Constraint Violation Counting**

**Violation Types.** A constraint violation occurs when the assistant's response contradicts an active constraint. We track four violation classes:

1. **Format violations:** Output form mismatches declared format (e.g., prose when code requested)
2. **Scope violations:** Content exceeds or undercuts specified boundaries (e.g., excessive detail when "brief summary" requested)
3. **Exclusion violations:** Forbidden elements appear in output (e.g., using a banned library)
4. **Requirement violations:** Mandatory elements are absent (e.g., missing error handling when explicitly required)

**Detection.** Violations are detected via:
- **Regex patterns:** Format constraints (code blocks, list structures, table formats) are verified via pattern matching
- **Keyword presence/absence:** Exclusion and requirement constraints use keyword lists
- **Heuristic rules:** Scope violations use token-count thresholds (±50% of requested length triggers violation)

**Penalty Assignment.** Each violation type contributes to \( V_t \):

\[
V_t = \sum_{i=1}^{4} n_i \cdot p_i
\]

Where \( n_i \) is the count of type-\( i \) violations and \( p_i \) is the penalty weight:

- Format: \( p_1 = 0.15 \) cu
- Scope: \( p_2 = 0.10 \) cu  
- Exclusion: \( p_3 = 0.20 \) cu
- Requirement: \( p_4 = 0.18 \) cu

Penalty weights were calibrated via operator surveys (Section 8.4) to reflect perceived severity of each violation type.

### **2.4 Recovery-Attempt Frequency**

**Definition.** A recovery attempt is a user turn that explicitly signals misalignment and attempts to re-establish the intent vector. This indicates that drift has become perceptible to the operator.

**Detection Markers.** Recovery attempts are identified by linguistic patterns:

**Negation phrases:**
- "No, I meant..."
- "That's not what I asked for"
- "Disregard that"
- "Let me rephrase"

**Re-anchoring phrases:**
- "To clarify:"
- "More specifically:"
- "The actual goal is:"
- "Let's refocus on..."

**Constraint restatements:**
- Verbatim repetition of prior constraints
- Addition of "Remember:" preambles
- Explicit rollback commands ("Go back to step X")

**Pattern Matching.** The recovery detector uses a two-stage process:
1. **Keyword filter:** Flag turns containing ≥1 marker phrase
2. **Context verification:** Confirm the turn is corrective (not just clarificational) via presence of negation + restatement

**Frequency Computation.** Recovery frequency \( R_t \) at turn \( t \) is:

\[
R_t = \frac{\sum_{i=t-k}^{t} \mathbb{1}[\text{recovery}(i)]}{k}
\]

where \( k = 5 \) turns (rolling window) and \( \mathbb{1}[\cdot] \) is the indicator function.

Rising \( R_t \) signals sustained drift: the operator is fighting against accumulating misalignment. This component captures the human-perceived drift that purely semantic measures might miss.

### **2.5 Weighting Coefficients**

The drift formula combines three signals:

\[
\Delta_t = \alpha \cdot d_{norm}(S_t, I_0) + \beta \cdot V_t + \gamma \cdot R_t
\]

**Coefficient Values (empirically optimized via validation data):**
- \( \alpha = 1.0 \) (semantic deviation is the primary component)
- \( \beta = 0.8 \) (constraint violations are strong but not dominant)
- \( \gamma = 1.2 \) (recovery attempts are weighted highest—they indicate operator-perceived drift)

**Optimization Method.** Coefficients were tuned to maximize correlation with expert coherence ratings (Section 8) via grid search over \( \alpha, \beta, \gamma \in [0.5, 1.5] \) in 0.1 increments. The chosen values yielded Pearson \( r = 0.82 \) between \( \Delta_{window} \) and expert scores.

### **2.6 Measurement Units and Ranges**

**Units.** Drift is expressed in **coherence units per turn (cu/turn)**. One coherence unit represents the semantic deviation equivalent to a single, isolated format-violation in a low-complexity task.

**Typical Ranges (observed in validation data):**
- \( \Delta < 0.05 \): Negligible drift, high stability
- \( 0.05 \leq \Delta < 0.12 \): Acceptable drift, coherence maintained
- \( 0.12 \leq \Delta < 0.15 \): Warning zone, collapse risk rising
- \( \Delta \geq 0.15 \): Critical drift, collapse imminent or occurring

**Temporal Resolution.** Drift is computed per turn but reported as a 3-turn exponentially-weighted moving average to reduce noise from single-turn anomalies.

### **2.7 Worked Example**

**Task:** Implement a binary search algorithm in Python.

**Turn 0 (Anchor):**
- User: "Write a binary search function. Requirements: iterative (not recursive), include docstring, handle empty arrays gracefully."
- Intent vector \( I_0 \) established. Constraints: {iterative, docstring, empty-array handling}.

**Turn 1:**
- Assistant: [Provides iterative implementation with docstring and empty-array check]
- \( d(S_1, I_0) = 0.08 \) (low semantic distance—output aligns with request)
- \( V_1 = 0 \) (all constraints satisfied)
- \( R_1 = 0 \) (no recovery markers)
- \( \Delta_1 = 1.0(0.08) + 0.8(0) + 1.2(0) = 0.08 \) cu/turn

**Turn 2:**
- User: "Add type hints."
- (Expansion of requirements, but within coherent evolution)
- \( d(S_2, I_0) = 0.12 \) (slightly more distant—task has evolved)
- \( V_2 = 0 \)
- \( R_2 = 0 \)
- \( \Delta_2 = 0.12 \) cu/turn

**Turn 3:**
- Assistant: [Provides recursive version with type hints, no docstring]
- \( d(S_3, I_0) = 0.31 \) (semantic drift—recursive approach contradicts anchor)
- \( V_3 = 0.15 + 0.18 = 0.33 \) cu (format violation: recursive; requirement violation: missing docstring)
- \( R_3 = 0 \) (user hasn't responded yet)
- \( \Delta_3 = 1.0(0.31) + 0.8(0.33) + 1.2(0) = 0.57 \) cu/turn

**Turn 4:**
- User: "No, keep it iterative like I originally said. And don't drop the docstring."
- \( R_4 = 1.0 \) (explicit recovery attempt)
- (This recovery signal will contribute to \( \Delta_5 \), but turn 4 is user-side so no assistant \( \Delta \) computed)

**Window-level drift** (EWMA over turns 1-3):
\[
\Delta_{window} = \frac{e^{0}(0.08) + e^{-1/3}(0.12) + e^{-2/3}(0.57)}{e^{0} + e^{-1/3} + e^{-2/3}} = 0.21 \text{ cu/turn}
\]

This exceeds the \( \theta = 0.15 \) threshold, triggering a coherence warning and indicating high collapse risk.

### **2.8 Instrumentation**

**Real-Time Computation.** The Caelusyn Platform computes \( \Delta_t \) after every assistant response:

1. Extract \( P_t, R_t, K_t \)
2. Generate embeddings via local sentence-transformers service
3. Compute \( d(S_t, I_0) \) using cached \( I_0 \) embedding
4. Run violation detectors (regex, keyword, heuristic)
5. Check recovery markers in \( P_t \)
6. Combine via weighted formula
7. Update EWMA buffer
8. Log raw and smoothed \( \Delta \) values

**UI Display.** The drift gauge is a semicircular arc (0-0.3 cu/turn range) color-coded:
- Green (0-0.05): "Stable"
- Yellow (0.05-0.12): "Nominal"
- Orange (0.12-0.15): "Warning"
- Red (0.15+): "Drift Alert"

Numeric \( \Delta \) value is displayed below the gauge. Historical \( \Delta \) is plotted as a line graph in the session dashboard.

---

## **3. Coherence Window Duration (W_c)**

**Definition.** A coherence window, \( W_c \), is a contiguous sequence of interaction turns during which drift remains below a defined threshold \( \theta \). The duration of \( W_c \) is measured in turns.

### **3.1 Formal Definition**

Let \( \{\Delta_t\} \) be the drift time-series for a session. A coherence window \( W_c^{(i)} \) is the maximal interval \( [t_{start}, t_{end}] \) such that:

1. **Drift constraint:** \( \Delta_t < \theta \) for all \( t \in [t_{start}, t_{end}] \)
2. **Stability requirement:** If \( \Delta_t \geq \theta \), it must return below \( \theta \) within 2 turns, or the window collapses
3. **Boundary markers:** Explicit FBP signals or task-segment transitions force window termination regardless of \( \Delta \)

The duration is:

\[
|W_c^{(i)}| = t_{end} - t_{start} + 1 \text{ turns}
\]

### **3.2 Threshold Derivation**

**Empirical Calibration.** The drift threshold \( \theta \) represents the point beyond which recovery cost exceeds continuation value. We derived \( \theta = 0.15 \) cu/turn through operator cost-benefit analysis.

**Recovery Cost Measurement.** For 40 recorded drift-recovery episodes, we measured:
- \( T_{recovery} \): Number of turns required to restore coherence after drift detection
- \( C_{cognitive} \): Operator-reported cognitive load (1-7 scale) during recovery
- \( C_{context} \): Tokens spent re-establishing context (summed across recovery turns)

**Continuation Value.** For the same episodes, we estimated:
- \( V_{progress} \): Information successfully retained from pre-recovery turns (percentage of partial results still usable)
- \( V_{trajectory} \): Whether the drift episode yielded useful exploratory information (binary: yes/no)

**Threshold Selection.** We computed the net recovery cost:

\[
\text{NetCost}(\Delta) = T_{recovery}(\Delta) + 0.1 \cdot C_{cognitive}(\Delta) + 0.001 \cdot C_{context}(\Delta) - V_{progress}(\Delta) - 2 \cdot V_{trajectory}(\Delta)
\]

And identified the \( \Delta \) value where \( \text{NetCost} \) transitions from negative to positive:

| Drift \( \Delta \) | \( T_{recovery} \) | NetCost | Decision |
|---------|------------|---------|----------|
| 0.10 | 1.2 turns | -0.8 | Continue (recovery is cheap) |
| 0.12 | 2.3 turns | -0.3 | Continue (marginal) |
| 0.15 | 3.8 turns | +0.6 | Reset (recovery is expensive) |
| 0.18 | 5.1 turns | +2.1 | Reset (recovery is very expensive) |

The crossover point is \( \theta \approx 0.15 \) cu/turn. This threshold minimizes total session cost (task turns + recovery turns) across all validation tasks.

**Sensitivity Analysis.** We tested \( \theta \in \{0.10, 0.12, 0.15, 0.18, 0.20\} \):

- \( \theta = 0.10 \): Excessive window resets (mean window duration = 4.2 turns), operators report "constant interruption"
- \( \theta = 0.12 \): Slightly premature resets, but acceptable (mean = 6.1 turns)
- \( \theta = 0.15 \): Optimal balance (mean = 8.7 turns), minimal operator complaints
- \( \theta = 0.18 \): Too permissive, allows costly drift episodes (mean = 11.3 turns, but 18% end in collapse requiring >5 recovery turns)
- \( \theta = 0.20 \): Severe drift undetected until catastrophic collapse

The \( \theta = 0.15 \) setting is robust across task types and operator skill levels.

### **3.3 Window Boundary Detection**

Windows terminate via three mechanisms:

**1. Drift Collapse (Implicit Boundary).** If \( \Delta_t \geq \theta \) for three consecutive turns, the window collapses at turn \( t \). The 3-turn requirement prevents single-turn noise from triggering false collapses.

**Collapse Condition:**

\[
\text{Collapse at } t \iff \Delta_{t-2}, \Delta_{t-1}, \Delta_t \geq \theta
\]

**2. Flow-Break Protocol (Explicit Boundary).** User-initiated FBP markers force immediate window termination:

**FBP Markers:**
- "Reset."
- "New vector:"
- "Let's start over."
- "Discard all that."
- "Begin again with..."

When an FBP marker is detected, the current window closes at the previous turn, and a new window begins immediately.

**3. Task-Segment Transition (Structural Boundary).** In HBTC chains, subtask completion often naturally resets the window. Structural transitions are detected via:

- **Delivery markers:** "Here is X.", "Completed.", "Output:"
- **New-directive markers:** New SVDP header follows immediately
- **Context-shift indicators:** Semantic distance between consecutive user prompts exceeds 0.4 (indicating topic change)

When a structural transition is detected, the window closes even if \( \Delta < \theta \). This prevents artificially long windows that span multiple distinct subtasks.

### **3.4 Nested and Overlapping Windows**

**Micro-Resets.** Operators sometimes issue brief FBP signals (e.g., "Clarify: X should be Y") that don't fully reset the interaction vector—they refine rather than replace the anchor. These create **nested windows**:

- The parent window \( W_c^{(parent)} \) remains open
- A child window \( W_c^{(child)} \) opens within it
- If the child window collapses, the parent window continues with adjusted \( I_0 \)

**Nesting Rules:**
- Child windows inherit constraints from parent \( K_{child} \supseteq K_{parent} \)
- Child window drift is computed relative to its own \( I_0^{(child)} \), not the parent's
- Maximum nesting depth: 2 levels (grandchild windows are not tracked)

**Detection.** Micro-resets are distinguished from full resets via linguistic markers:
- Micro: "Clarify:", "Adjust:", "Refine:"
- Full: "Reset.", "Start over.", "New task:"

In validation data, 12% of windows contained nested child windows (mean depth: 1 child per parent).

### **3.5 Window Statistics**

**Duration Metrics.** For a session with \( N \) windows, we compute:

**Mean Window Duration:**
\[
\overline{W_c} = \frac{1}{N} \sum_{i=1}^{N} |W_c^{(i)}|
\]

**Window Stability:** Proportion of session spent in coherent windows:
\[
\text{Stability} = \frac{\sum_{i=1}^{N} |W_c^{(i)}|}{\text{Total Session Turns}}
\]

**Collapse Frequency:** Average turns between collapses:
\[
\text{MTBC} = \frac{\text{Total Turns}}{N-1}
\]
(Mean Time Between Collapses, analogous to MTBF in reliability engineering)

### **3.6 Visualization**

**Timeline Representation.** Each window is rendered as a horizontal bar on a timeline:
- **X-axis:** Turn index (0 to session end)
- **Y-axis:** Window ID (stacked vertically)
- **Bar color:** Gradient from green (low average \( \Delta \)) to red (high average \( \Delta \))
- **Bar height:** Fixed (0.8 units) for primary windows, 0.4 units for nested child windows
- **Boundary markers:** Vertical dashed lines at collapse points (red) or FBP resets (blue)

**Interactive Features (Caelusyn Platform):**
- Hover over bar → tooltip shows window statistics (\( |W_c| \), mean \( \Delta \), terminal \( \Delta \), collapse cause)
- Click bar → highlight corresponding transcript segment
- Click boundary → display the turn that triggered collapse or reset

**Interpretive Aid.** Long, green bars indicate stable, high-coherence windows. Short, red bars indicate fragmented, drift-prone interaction. A session with many short windows suggests either high task complexity or operator inexperience (or both).

### **3.7 Worked Example**

**Session:** UI component specification (24 turns total).

**Window 1:** Turns 0-8
- \( I_0 \): "Design a modal dialog component. Must include close button, backdrop click-to-close, and focus trap."
- Mean \( \Delta = 0.07 \) cu/turn
- Terminal \( \Delta = 0.09 \) cu/turn
- **Closure cause:** Structural transition (subtask complete: modal spec delivered, new task: button component)

**Window 2:** Turns 9-13
- \( I_0 \): "Now design the button component. States: default, hover, active, disabled."
- Mean \( \Delta = 0.11 \) cu/turn
- Terminal \( \Delta = 0.16 \) cu/turn
- **Closure cause:** Drift collapse (assistant added unnecessary loading state, violating scope constraint)

**Recovery Period:** Turns 14-15 (not in any window)
- Turn 14: User: "No loading state. Just the four states I listed."
- Turn 15: Assistant provides corrected spec.

**Window 3:** Turns 16-23
- \( I_0 \): Re-anchored button spec with explicit exclusion of loading state.
- Mean \( \Delta = 0.06 \) cu/turn
- Terminal \( \Delta = 0.08 \) cu/turn
- **Closure cause:** Session end (task complete)

**Session Statistics:**
- \( N = 3 \) windows
- \( \overline{W_c} = (9 + 5 + 8) / 3 = 7.3 \) turns
- Stability \( = 22 / 24 = 91.7\% \)
- MTBC \( = 24 / 2 = 12 \) turns

---

## **4. Interaction Bandwidth (B)**

**Definition.** Interaction bandwidth, \( B \), is the effective information‑transfer rate sustained within a coherence window \( W_c \). It measures how much stabilized semantic content is exchanged per turn when the interaction is coherent.

### **4.1 Formal Definition**

\[
B = \frac{S}{T}
\]

Where:
- \( S \) = total stabilized semantic content (in compressed semantic units, **csu**) delivered within \( W_c \)
- \( T \) = turn count within \( W_c \)

**Units:** Compressed semantic units per turn (csu/turn).

### **4.2 Stabilized Semantic Content (S)**

**Conceptual Foundation.** Not all output tokens carry equal information value. Stabilized semantic content is the subset of output that:
1. Directly advances the anchored intent \( I_0 \)
2. Can be used in subsequent turns without clarification or correction
3. Reduces uncertainty about the task state

**Measurement Components.** \( S \) is computed as:

\[
S = \sum_{t \in W_c} \left[ H_{reduction}(t) + C_{reuse}(t) + E_{compression}(t) \right]
\]

Where:
- \( H_{reduction}(t) \): Entropy reduction at turn \( t \)
- \( C_{reuse}(t) \): Context-reuse efficiency at turn \( t \)
- \( E_{compression}(t) \): Compression efficiency at turn \( t \)

#### **4.2.1 Entropy Reduction**

**Definition.** Entropy reduction quantifies how much the assistant's response narrows the space of possible next states.

**Computation.** We estimate conditional entropy using a language model (GPT-2 small, frozen weights):

\[
H_{reduction}(t) = H(\text{continuation} \mid \text{generic context}) - H(\text{continuation} \mid \text{anchored context})
\]

Where:
- **Generic context:** "Continue the following interaction:" + last 2 turns
- **Anchored context:** SVDP anchor + constraint list + last 2 turns

We measure entropy via average per-token log-probability over a 50-token continuation generated by the entropy-estimation model:

\[
H = -\frac{1}{50} \sum_{i=1}^{50} \log P(\text{token}_i \mid \text{context})
\]

**Normalization.** Raw entropy reduction is scaled to [0, 1]:

\[
H_{reduction,norm} = \frac{H_{reduction,raw}}{H_{max}}
\]

where \( H_{max} = 3.5 \) nats (observed maximum in validation data). One unit of normalized entropy reduction equals one csu.

**Interpretation:** High \( H_{reduction} \) means the response is **directive-aligned**—it constrains future possibilities in a way that matches the task requirements. Low \( H_{reduction} \) means the response is generic or off-topic.

#### **4.2.2 Context Reuse Efficiency**

**Definition.** Context reuse measures how effectively the response leverages prior established content without redundant re-explanation.

**Detection.** We identify **reference markers** in the assistant's response:
- Anaphoric references ("this", "that", "the aforementioned")
- Explicit back-references ("as defined in step 2", "using the function from earlier")
- Implicit continuations (technical terms introduced earlier, used without re-definition)

**Scoring.** For each turn \( t \):

\[
C_{reuse}(t) = \frac{N_{references}}{N_{tokens}} \cdot \alpha_{novelty}
\]

Where:
- \( N_{references} \) = count of reference markers
- \( N_{tokens} \) = total response tokens
- \( \alpha_{novelty} \) = novelty bonus (1.5 if the response introduces new content; 1.0 if purely reiterative)

**Interpretation:** High \( C_{reuse} \) indicates the response is **building on** prior context rather than restarting. This is characteristic of high-bandwidth interaction.

#### **4.2.3 Compression Efficiency**

**Definition.** Compression efficiency measures how much output is generated per unit of directive input.

\[
E_{compression}(t) = \frac{\text{tokens}_{response}(t)}{\text{tokens}_{prompt}(t)} \cdot w_{quality}
\]

Where \( w_{quality} \) is a quality weight:
- \( w_{quality} = 1.0 \) if the response satisfies all constraints
- \( w_{quality} = 0.5 \) if minor violations occur
- \( w_{quality} = 0.0 \) if the response is off-topic or requires full redo

**Normalization.** Values above 10 are clamped (to handle single-word prompts yielding long outputs):

\[
E_{compression,norm} = \min(E_{compression}, 10)
\]

**Interpretation:** High \( E_{compression} \) means the user can generate significant output with minimal input—the signature of SVDP-style compressed directives.

### **4.3 Bandwidth Calculation**

**Per-Window Bandwidth.** For a window \( W_c \) spanning turns \( [t_1, t_2] \):

\[
B_{W_c} = \frac{\sum_{t=t_1}^{t_2} \left[ H_{reduction}(t) + C_{reuse}(t) + E_{compression}(t) \right]}{t_2 - t_1 + 1}
\]

**Session-Level Bandwidth.** Average over all windows:

\[
B_{session} = \frac{1}{N} \sum_{i=1}^{N} B_{W_c^{(i)}}
\]

### **4.4 Bandwidth Collapse**

**Collapse Indicator.** Bandwidth collapse is detected when \( B \) drops below \( 0.5 \cdot B_{baseline} \) for 3 consecutive turns, where \( B_{baseline} \) is the moving average over the prior 10 turns.

**Collapse Mechanisms.** Three patterns precede collapse:

1. **Repetition Spiral:** \( C_{reuse} \to 0 \) (responses stop referencing prior context), \( H_{reduction} \to 0 \) (responses become generic)
2. **Clarification Cascade:** \( E_{compression} \to 0 \) (responses are verbose but empty, requiring follow-up clarifications)
3. **Constraint Flood:** \( w_{quality} \to 0 \) (responses violate constraints, yielding zero effective content)

**Recovery Pattern.** Bandwidth typically recovers 1-2 turns after an FBP reset or explicit re-anchoring. The recovery rate is:

\[
\text{Recovery Rate} = \frac{B_{post-reset} - B_{pre-collapse}}{N_{recovery\_turns}}
\]

In validation data, expert operators achieve recovery rates of 0.8-1.2 csu/turn², while novices show 0.3-0.5 csu/turn².

### **4.5 Typical Bandwidth Ranges**

**Observed Values (validation data):**

| Interaction Quality | Bandwidth (csu/turn) | Characteristics |
|---------------------|----------------------|-----------------|
| Excellent | \( B > 2.5 \) | SVDP-driven, minimal clarification, high compression |
| Good | \( 1.5 \leq B \leq 2.5 \) | Occasional drift-correction, generally coherent |
| Acceptable | \( 1.0 \leq B < 1.5 \) | Frequent clarification, moderate efficiency |
| Poor | \( 0.5 \leq B < 1.0 \) | High repetition, low reuse, verbose exchanges |
| Collapsed | \( B < 0.5 \) | Incoherent, predominantly recovery attempts |

**Task-Specific Baselines:**

| Task Type | Mean \( B \) | Std Dev |
|-----------|--------------|---------|
| Algorithm implementation | 2.1 | 0.4 |
| Research synthesis | 1.8 | 0.5 |
| UI specification | 2.3 | 0.3 |
| Project planning | 1.6 | 0.6 |

UI specification tasks achieve highest bandwidth due to structured, template-driven SVDP usage. Project planning shows highest variance due to exploratory vs. directive interaction styles.

### **4.6 Real-Time Monitoring**

**Dashboard Display.** The Caelusyn Platform renders bandwidth as:
- **Bar chart:** Last 20 turns, with current-window turns highlighted
- **Trend line:** 10-turn moving average
- **Collapse alert:** Red flash when \( B \) drops below collapse threshold

**Audio Feedback.** Optional audio cues:
- Rising pitch sequence: bandwidth increasing
- Falling pitch sequence: bandwidth decreasing
- Alert tone: collapse detected

### **4.7 Worked Example**

**Task:** Research synthesis (3 papers on reinforcement learning).

**Window:** Turns 0-6.

**Turn 0 (User Prompt):**
"Summarize the key contribution of each paper. Format: bullet list, one bullet per paper, max 15 words each."

**Turn 1 (Assistant Response):**
"• Paper 1: Introduces DQN, combining Q-learning with deep neural networks for Atari.  
• Paper 2: Proposes PPO, improving policy gradient stability via clipping.  
• Paper 3: Applies multi-agent RL to traffic signal optimization in simulated cities."

**Bandwidth Computation for Turn 1:**

\( H_{reduction} \):
- Generic context entropy: 4.2 nats
- Anchored context entropy: 2.1 nats
- \( H_{reduction,raw} = 4.2 - 2.1 = 2.1 \) nats
- \( H_{reduction,norm} = 2.1 / 3.5 = 0.60 \) csu

\( C_{reuse} \):
- \( N_{references} = 3 \) (implicit references to "Paper 1", "Paper 2", "Paper 3")
- \( N_{tokens} = 42 \)
- \( \alpha_{novelty} = 1.5 \) (introduces new content: DQN, PPO, traffic)
- \( C_{reuse} = (3/42) \cdot 1.5 = 0.11 \) csu

\( E_{compression} \):
- \( \text{tokens}_{response} = 42 \)
- \( \text{tokens}_{prompt} = 18 \)
- \( w_{quality} = 1.0 \) (all constraints satisfied)
- \( E_{compression} = (42/18) \cdot 1.0 = 2.33 \)
- \( E_{compression,norm} = 2.33 \) csu (below clamp)

**Total for Turn 1:**
\( S_1 = 0.60 + 0.11 + 2.33 = 3.04 \) csu

**Subsequent turns (abbreviated):**
- Turn 2: Compare methodologies → \( S_2 = 2.80 \) csu
- Turn 3: Identify common limitations → \( S_3 = 2.65 \) csu
- Turn 4: Suggest synthesis framework → \( S_4 = 2.40 \) csu
- Turn 5: Clarification (model misinterpreted "common") → \( S_5 = 0.90 \) csu
- Turn 6: Corrected synthesis → \( S_6 = 2.95 \) csu

**Window Bandwidth:**
\[
B_{W_c} = \frac{3.04 + 2.80 + 2.65 + 2.40 + 0.90 + 2.95}{6} = \frac{14.74}{6} = 2.46 \text{ csu/turn}
\]

This is "Good" bandwidth (1.5-2.5 range), indicating efficient, coherent interaction despite the one clarification turn.

---

## **5. Compression–Expansion Ratio (R_ce)**

**Definition.** The compression–expansion ratio, \( R_{ce} \), is the ratio of compressive to expansive turns within a coherence window. It quantifies the structural rhythm of the interaction.

### **5.1 Formal Definition**

\[
R_{ce} = \frac{C}{E}
\]

Where:
- \( C \) = count of compression turns in the window
- \( E \) = count of expansion turns in the window

**Neutral Turns.** Turns classified as neutral (neither compression nor expansion) are excluded from the ratio. The effective turn count is \( C + E \), not the total window length.

### **5.2 Turn Classification**

Each user prompt and assistant response is independently classified as Compression (C), Expansion (E), or Neutral (N).

#### **5.2.1 Compression Turns**

**Definition.** Compression turns reduce ambiguity, narrow focus, or consolidate prior content.

**Linguistic Markers:**

**User-side compression:**
- "So we have X and Y, and need Z."
- "To clarify, only use method A."
- "Recap: steps 1-3 are done."
- "Focus on the core requirement."
- "Strip out the extras."

**Assistant-side compression:**
- "Summarizing the above:"
- "In essence:"
- "The key point is:"
- "Consolidating:"
- Delivery of final output after iterative refinement

**Structural Indicators:**
- Token count < 0.7 × moving average (terse, focused)
- High constraint-keyword density (≥3 constraint terms per 100 tokens)
- Numbered list → single statement consolidation

**Semantic Indicators (embedding-based):**
- Cosine similarity to prior turn > 0.85 (restating/confirming)
- Low entropy increase (not introducing new concepts)

#### **5.2.2 Expansion Turns**

**Definition.** Expansion turns introduce new possibilities, explore alternatives, or elaborate on existing concepts.

**Linguistic Markers:**

**User-side expansion:**
- "What if we also consider Y?"
- "Could you explain the underlying mechanism?"
- "Let's add another test case."
- "Explore alternative approaches."
- "What are the edge cases?"

**Assistant-side expansion:**
- "Additionally, we could..."
- "Alternative approaches include:"
- "This relates to..."
- Detailed explanations of background concepts
- Exploration of multiple solution paths

**Structural Indicators:**
- Token count > 1.3 × moving average (verbose, elaborative)
- Introduction of new technical terms (≥2 new terms not in prior context)
- Conditional branching ("If X, then...; otherwise...")

**Semantic Indicators:**
- Cosine similarity to prior turn < 0.60 (introducing new concepts)
- High entropy increase (broadening conceptual space)

#### **5.2.3 Neutral Turns**

**Definition.** Neutral turns continue the current vector without clear directional shift.

**Characteristics:**
- Simple continuations: "Continue.", "Next.", "Proceed."
- Direct implementations of already-specified requirements
- Acknowledgments: "Got it.", "Understood."
- Token count within ±30% of moving average
- Cosine similarity to prior turn in [0.60, 0.85]

**Typical Proportion:** In validation data, 20-30% of turns are neutral.

#### **5.2.4 Classification Algorithm**

**Two-Stage Classifier:**

**Stage 1: Rule-Based Filter**
1. Check for explicit compression/expansion markers (keyword lists)
2. If marker found, classify accordingly (confidence: high)

**Stage 2: Feature-Based Classifier** (if no markers found)
1. Compute structural features (token ratio, keyword density, new-term count)
2. Compute semantic features (cosine similarity, entropy change)
3. Apply logistic regression classifier (trained on 500 hand-labeled turns)
4. If probability > 0.7, assign class; otherwise, classify as Neutral

**Inter-Annotator Agreement.** Two independent raters achieved Cohen's κ = 0.78 on 200-turn test set, indicating substantial agreement.

### **5.3 Ratio Calculation**

**Per-Window R_ce.** For window \( W_c \):

\[
R_{ce}(W_c) = \frac{\sum_{t \in W_c} \mathbb{1}[\text{class}(t) = C]}{\sum_{t \in W_c} \mathbb{1}[\text{class}(t) = E]}
\]

If \( E = 0 \) (no expansion turns), \( R_{ce} = \infty \) (pure compression). If \( C = 0 \), \( R_{ce} = 0 \) (pure expansion).

**Rolling R_ce.** For real-time monitoring, compute over a sliding window of \( k = 10 \) turns (excluding neutral):

\[
R_{ce,rolling}(t) = \frac{\sum_{i=t-k}^{t} \mathbb{1}[\text{class}(i) = C]}{\sum_{i=t-k}^{t} \mathbb{1}[\text{class}(i) = E]}
\]

### **5.4 Optimal Ratio by Task Type**

**Empirical Baselines (from validation data):**

| Task Type | Optimal \( R_{ce} \) | Rationale |
|-----------|----------------------|-----------|
| Algorithm implementation | 1.8 - 2.5 | Heavy compression: spec → code, minimal exploration |
| Research synthesis | 0.6 - 1.2 | Balanced: exploration of sources + consolidation of findings |
| UI specification | 2.0 - 3.0 | Highly structured: iterate toward precise spec |
| Project planning | 0.4 - 0.8 | Expansion-heavy: brainstorm tasks, then light compression |
| Creative writing | 0.2 - 0.5 | Very expansion-heavy: idea generation dominates |

**Interpretation:**
- **Implementation tasks** require high compression (turning broad requirements into concrete code)
- **Exploratory tasks** require low compression (generating options before selecting)
- **Specification tasks** require moderate-to-high compression (refinement toward precision)

### **5.5 Deviation and Drift Correlation**

**Key Finding:** Deviation from a session's established baseline \( R_{ce} \) predicts drift onset.

**Baseline Calculation.** The session baseline is the moving average \( R_{ce} \) over the first 15 turns (or first window, whichever is longer):

\[
R_{ce,baseline} = \text{mean}\left( R_{ce,rolling}(1), R_{ce,rolling}(2), \ldots, R_{ce,rolling}(15) \right)
\]

**Deviation Metric:**

\[
\delta_{R_{ce}}(t) = \left| R_{ce,rolling}(t) - R_{ce,baseline} \right|
\]

**Drift Prediction.** In validation data:
- \( \delta_{R_{ce}} > 0.4 \cdot R_{ce,baseline} \) for 3 consecutive turns predicted \( \Delta \geq 0.12 \) within the next 5 turns (precision = 0.74, recall = 0.68)
- Sudden expansion spikes (\( R_{ce,rolling} < 0.5 \cdot R_{ce,baseline} \)) were the strongest predictor of imminent drift (precision = 0.81)

**Mechanism.** Excessive expansion without consolidation fragments the interaction:
- The operator introduces too many alternative paths
- The assistant explores multiple directions without closure
- The anchored intent \( I_0 \) becomes ambiguous (which path is primary?)
- Drift accumulates as the interaction loses focus

**Prescriptive Implication.** When \( \delta_{R_{ce}} \) exceeds threshold, the platform suggests: "Consider consolidating with a compression turn (e.g., 'So our priority is X')."

### **5.6 Visualization**

**Line Plot.** Rolling \( R_{ce} \) plotted over turn index:
- **X-axis:** Turn index
- **Y-axis:** \( R_{ce,rolling} \)
- **Reference line:** \( R_{ce,baseline} \) (horizontal dashed line)
- **Alert zones:** Shaded regions where \( \delta_{R_{ce}} > 0.4 \cdot R_{ce,baseline} \) (yellow fill)

**Stacked Bar Chart (alternative).** For each window \( W_c \):
- Bar height = total classified turns (\( C + E \))
- Bar segments: green (compression), blue (expansion)
- Segment heights indicate \( C \) and \( E \) counts
- Ratio displayed as text label

### **5.7 Worked Example**

**Task:** Project planning (decompose "Build a recipe app").

**First 10 turns (classified):**

| Turn | Type | Class | Rationale |
|------|------|-------|-----------|
| 0 (U) | Prompt | E | "What are all the features we'd need?" (exploration) |
| 1 (A) | Response | E | Lists 8 possible features (expansion) |
| 2 (U) | Prompt | E | "What about social features?" (further expansion) |
| 3 (A) | Response | E | Details 4 social features (expansion) |
| 4 (U) | Prompt | C | "Focus on core features only: recipes, search, save." (compression) |
| 5 (A) | Response | C | Restates trimmed feature set (compression) |
| 6 (U) | Prompt | N | "Break those into tasks." (neutral continuation) |
| 7 (A) | Response | E | Generates 12 tasks with dependencies (expansion) |
| 8 (U) | Prompt | C | "Group into phases." (compression) |
| 9 (A) | Response | C | 3-phase structure (compression) |

**Counts:**
- \( C = 4 \) (turns 4, 5, 8, 9)
- \( E = 5 \) (turns 0, 1, 2, 3, 7)
- \( N = 1 \) (turn 6)

**Ratio:**
\[
R_{ce} = \frac{4}{5} = 0.80
\]

This is within the optimal range (0.4-0.8) for project planning, indicating appropriate balance of exploration and consolidation.

**Subsequent Drift Episode (turns 11-14):**

| Turn | Class | Rationale |
|------|-------|-----------|
| 11 (U) | E | "Should we add gamification?" |
| 12 (A) | E | Describes 5 gamification mechanics |
| 13 (U) | E | "What about monetization?" |
| 14 (A) | E | Details 3 revenue models |

**Updated counts (turns 5-14, rolling window):**
- \( C = 2 \)
- \( E = 7 \)
- \( R_{ce,rolling} = 2/7 = 0.29 \)

**Deviation:**
\[
\delta_{R_{ce}} = |0.29 - 0.80| = 0.51 > 0.4 \cdot 0.80 = 0.32
\]

Alert triggered: "Expansion spike detected. Consider refocusing on core plan."

**Observed Outcome:** \( \Delta \) rose from 0.09 to 0.14 by turn 16, approaching drift threshold. Operator issued compression turn at 17 ("Let's stick to the 3-phase plan we had"), recovering coherence.

---

## **6. Instrumentation in the Caelusyn Platform**

The Caelusyn Research Platform implements all four parameters in a unified instrumentation layer, providing real-time feedback and comprehensive session analytics.

### **6.1 Architecture Overview**

**Core Components:**

1. **Session State Manager (SSM):** Persistent object maintaining:
   - Anchor stack (intent vectors \( I_0 \) for current and nested windows)
   - Active constraint set \( K \)
   - Current window ID and boundary metadata
   - Rolling parameter buffers (\( \Delta \), \( B \), \( R_{ce} \) time-series)

2. **Parameter Computation Engine (PCE):** After each assistant response:
   - Invokes embedding service for semantic vectors
   - Runs violation detectors (regex, keyword, heuristic)
   - Executes turn classifier (C/E/N)
   - Computes entropy reduction via GPT-2 service
   - Updates all four parameters
   - Logs raw and derived values

3. **Window Manager (WM):** Monitors:
   - Drift threshold crossings
   - FBP marker detection
   - Structural transition signals
   - Manages window stack (open/close/nest operations)

4. **Dashboard Renderer (DR):** Real-time UI display:
   - Drift gauge (arc visualization)
   - Bandwidth bar chart
   - \( R_{ce} \) line plot
   - Window timeline
   - Alert notifications

### **6.2 Data Flow**

**Per-Turn Cycle:**


User submits prompt
  ↓
SSM updates anchor stack (if SVDP/FBP detected)
  ↓
Model generates response
  ↓
PCE extracts text, computes embeddings
  ↓
PCE runs all detectors and classifiers
  ↓
PCE computes Δ, B, R_ce for current turn
  ↓
WM checks window boundary conditions
  ↓
WM updates window stack if needed
  ↓
SSM logs all parameters to session database
  ↓
DR updates UI visualizations
  ↓
Alerts triggered if thresholds exceeded


**Latency:** Typical per-turn processing: 80-150ms (on M1 MacBook Pro). The embedding and entropy steps dominate (60-100ms).

### **6.3 Real-Time Drift Alerting**

**Drift Gauge (Visual):**
- **Type:** Semicircular arc (speedometer style)
- **Range:** 0 to 0.30 cu/turn
- **Needle:** Current \( \Delta_{window} \) (EWMA)
- **Color zones:**
  - Green (0-0.05): "Stable"
  - Yellow (0.05-0.12): "Nominal"
  - Orange (0.12-0.15): "Warning"
  - Red (0.15-0.30): "Drift Alert"
- **Numeric display:** Below gauge, two decimal places

**Audio Cues (Optional):**
- **Warning (Δ > 0.10):** Subtle chime (single beep, 800 Hz, 200ms)
- **Alert (Δ > 0.15):** Distinct alert (double beep, 1200 Hz, 150ms each)
- **Collapse:** Continuous tone (3 beeps, 600 Hz, 300ms each)

Audio can be toggled via settings. Validation operators reported audio as "helpful but occasionally distracting" (60% positive, 40% disabled it).

### **6.4 Window Collapse Detection UI**

**Transcript View:**
- When a window collapses, a visual separator is inserted:
  - Horizontal red dashed line
  - Timestamp of collapse
  - Collapse cause label ("Drift threshold exceeded" / "FBP reset" / "Task transition")

**Collapse Notification:**
- Toast notification (top-right corner): "Window collapsed at turn X. Reason: [cause]."
- Clickable to jump to collapse point in transcript

**Auto-Suggestions:**
- Upon collapse, the platform suggests recovery actions:
  - "Consider re-anchoring with an SVDP directive."
  - "Review constraints—possible conflict detected."
  - "Summary available—click to recap current state."

### **6.5 Bandwidth Dashboard**

**Bar Chart (Last 20 Turns):**
- **X-axis:** Turn index (relative: -20 to 0)
- **Y-axis:** Bandwidth \( B \) (0 to 4 csu/turn)
- **Bar colors:**
  - Blue: Turns within current window
  - Gray: Turns outside current window
  - Red outline: Turns where collapse occurred
- **Trend line:** 10-turn moving average (dashed line overlay)

**Bandwidth Indicator (Numeric):**
- Current-window average \( B \)
- Color-coded background (green/yellow/red per Table in 4.5)
- Sparkline (mini time-series) next to numeric value

**Collapse Alert:**
- When \( B < 0.5 \cdot B_{baseline} \) for 3 turns:
  - Bar chart flashes red border
  - Text alert: "Bandwidth collapse detected—information transfer is degraded."

### **6.6 R_ce Monitoring**

**Line Plot (Last 20 Turns):**
- **X-axis:** Turn index
- **Y-axis:** \( R_{ce,rolling} \) (0 to 5)
- **Reference line:** \( R_{ce,baseline} \) (horizontal dashed line)
- **Alert zones:** Shaded yellow when \( |\delta_{R_{ce}}| > 0.4 \cdot R_{ce,baseline} \)
- **Point markers:** Color-coded by turn type (green=C, blue=E, gray=N)

**Deviation Alert:**
- When deviation threshold exceeded:
  - Text suggestion: "Consider a compression turn to refocus."
  - Highlight the most recent expansion turns (clickable to review)

### **6.7 Exportable Session Logs**

**JSON Export Format:**


json
{
  "session_id": "uuid-string",
  "timestamp_start": "ISO-8601",
  "timestamp_end": "ISO-8601",
  "total_turns": 42,
  "windows": [
    {
      "window_id": 1,
      "start_turn": 0,
      "end_turn": 12,
      "duration": 13,
      "mean_drift": 0.08,
      "terminal_drift": 0.11,
      "mean_bandwidth": 2.3,
      "mean_Rce": 1.8,
      "closure_cause": "structural_transition"
    },
    ...
  ],
  "turns": [
    {
      "turn_index": 0,
      "role": "user",
      "content": "...",
      "timestamp": "ISO-8601",
      "window_id": 1,
      "drift": 0.05,
      "bandwidth": 2.1,
      "Rce_rolling": 1.5,
      "class": "E",
      "violations": [],
      "recovery_markers": []
    },
    ...
  ],
  "parameters_timeseries": {
    "drift": [0.05, 0.07, 0.06, ...],
    "bandwidth": [2.1, 2.3, 2.2, ...],
    "Rce": [1.5, 1.6, 1.8, ...]
  }
}


**CSV Export (alternative):**
- Flat table: turn_index, role, drift, bandwidth, Rce, window_id, class, violations
- Suitable for analysis in R/Python

**Use Cases:**
- Offline analysis and visualization
- Operator training (review sessions to identify improvement opportunities)
- Research corpus (aggregate statistics across tasks/operators)

### **6.8 Implementation Notes**

**Embedding Service:**
- Local sentence-transformers server (runs on port 5001)
- Handles ~50 requests/sec on M1 hardware
- GPU acceleration not required (CPU-only is adequate)

**Entropy Estimation Service:**
- GPT-2 small model (124M parameters) via HuggingFace Transformers
- Runs on port 5002
- Invoked once per turn (low overhead)

**Turn Classifier:**
- Logistic regression model (scikit-learn)
- Trained on 500 labeled turns (manually annotated)
- Serialized via joblib, loaded at platform startup

**Database:**
- SQLite for session logs (lightweight, local-first)
- JSON export generated on-demand

**Frontend:**
- React + D3.js for visualizations
- WebSocket connection for real-time updates

---

## **7. Experimental Methods and Parameter Validation**

This section details the experimental design and statistical validation of the coherence parameters introduced in Section 6. It describes how the parameters were computed, how operator performance was sampled and rated, and the metrics used to establish validity.

### **7.1 Participants and Task Design**

**Operators**

- 9 human participants (3 novice, 3 competent, 3 expert), selected based on prior interaction hours with large language models (LLMs).
- Skill classification follows Table 4 (see Appendix).

**Tasks**

Each operator completed four distinct task types (Table 1):

1. Algorithm implementation  
2. Research synthesis  
3. UI specification  
4. Project planning  

**Run conditions**

Each task was performed twice:

- **Baseline:** Standard LLM interface (no coherence instrumentation).  
- **Instrumented:** Interface with real-time coherence feedback (Caelusyn Platform).  

**Total sessions:** 72 (9 operators × 4 tasks × 2 runs)

> Note (recommended control): Counterbalance task order and condition order (baseline vs instrumented) to reduce learning and fatigue effects.

### **7.2 Parameter Computation**

**Live metrics computed per turn**

- Drift magnitude (Δ)  
- Bandwidth (B)  
- Compression–expansion ratio (R_ce)  
- Coherence window duration (W_c)

**Computation method**

- Full implementation details appear in Appendix B (Software Implementation).
- Drift computed using cosine distance from the intent anchor.
- Bandwidth estimated from entropy reduction and content reuse.
- \(R_{ce}\) computed from classifier outputs.
- Coherence windows (\(W_c\)) defined by drift collapse and re-anchoring events.

### **7.3 Session Logging and Data Collection**

**Logged per session**

- Full transcript (prompts, completions, timestamps)
- Real-time parameter values (Δ, B, \(R_{ce}\), \(W_c\))
- Completion time (turns and wall-clock minutes)
- Post-session subjective ratings (Section 7.4)

### **7.4 Subjective Ratings**

Operators completed a 7-point Likert questionnaire post-session:

1. Smoothness  
2. Cognitive load  
3. Frustration  
4. Output quality  
5. Efficiency  

For instrumented runs:

6. Drift gauge usefulness  
7. Bandwidth chart usefulness  
8. \(R_{ce}\) plot usefulness  

### **7.5 Expert Coherence Ratings**

Two independent raters (PhD-level HCI/NLP background) scored each session (blinded to condition) on:

- Constraint adherence (1–5)  
- Narrative continuity (1–5)  
- Clarity of progression (1–5)  

**Overall coherence score**

\[
\text{Coherence} = \frac{\text{Constraint} + \text{Continuity} + \text{Progression}}{3}
\]

Inter-rater reliability (ICC) was computed; see Section 8.1 and Table 6.

### **7.6 Validation Hypotheses and Statistical Methods**

We tested the following null hypotheses:

- **H₀₁:** Δ is uncorrelated with expert coherence score  
- **H₀₂:** B is uncorrelated with task completion time  
- **H₀₃:** \(R_{ce}\) is uncorrelated with subjective smoothness  
- **H₀₄:** \(W_c\) is uncorrelated with operator frustration  

**Statistical tests**

- Pearson correlation (continuous variables)  
- Spearman correlation (ordinal variables)  
- Bonferroni correction for multiple comparisons  

---

## **8. Results and Validation Findings**

### **8.1 Inter-Rater Reliability**

Expert coherence ratings showed high reliability (ICC = 0.82). See Table 6.

### **8.2 Drift Predicts Window Collapse**

- Sustained Δ ≥ 0.12 for ≥3 turns predicts collapse (sensitivity: 89.5%).
- Fisher’s exact test: p < 0.01.
- Conclusion: Δ rise is a significant early-warning signal.

### **8.3 Bandwidth Stability Across Windows**

- Bandwidth is stable within windows and drops sharply at boundaries.
- Coefficient of variation (CV) within window: 0.18; across boundaries: 0.52.
- Paired t-test: p < 0.001.
- Conclusion: Bandwidth is a robust coherence indicator.

### **8.4 \(R_{ce}\) Deviations Correlate with Drift**

- High-drift sessions show greater \(R_{ce}\) deviation.
- t-test: p < 0.001; Cohen’s d = 3.1.
- Directional bias toward expansion (78% of deviations).
- \(R_{ce}\) volatility predicts Δ spikes (F1 = 0.71).

### **8.5 Operator Skill Differences**

- Expert operators show lower Δ and shorter drift recovery time.
- ANOVA confirms significant differences across skill levels (p < 0.001).
- Bandwidth does not differ significantly (p = 0.07).
- Conclusion: Drift regulation, not speed, differentiates skill.

### **8.6 Correlation with External Metrics**

| Parameter            | External metric   | Correlation | p-value |
|---------------------|-------------------|------------:|:-------|
| Δ                   | Expert coherence  | r = -0.82   | <0.001 |
| B                   | Task time         | r = -0.67   | <0.001 |
| \(W_c\)             | Smoothness        | ρ = 0.71    | <0.001 |
| \(R_{ce}\) volatility | Frustration     | ρ = 0.64    | <0.001 |

**Conclusion:** All null hypotheses were rejected. Coherence parameters correlate strongly with interaction quality.

### **8.7 Baseline vs. Instrumented Runs**

- Instrumented runs reduce drift, especially for competent operators.
- Novices showed distraction effects; experts were near ceiling.
- Dashboard usefulness ratings were highest for experts (especially the \(R_{ce}\) chart).

---

## **9. Limitations and Boundary Conditions**

- Parameters reflect interaction *process*, not final output quality.
- Real-time instrumentation is required; approximation from transcripts is lossy.
- Parameters are platform-specific in implementation.
- These measures are not cognitive ability metrics; they describe dyad dynamics.

**Minimum reliable session length:** 15 turns.

---
## **10. Forward to Paper 3 (Applied Benefits)**

With a validated measurement layer established, **Paper 3** investigates applied consequences and intervention studies:

### **10.1 Research Questions for Paper 3**

**RQ1: Coherence-Aware Interaction Outcomes**
- Does real-time drift feedback reduce task completion time?
- Does bandwidth monitoring improve output quality (as measured by domain-specific metrics)?
- Are coherence-optimized sessions more efficient (output value per turn)?

**RQ2: Trainability of Drift Regulation**
- Can novice operators be taught to regulate drift?
- What is the learning curve (sessions required to achieve expert-level \( \Delta \))?
- Do trained operators retain skills across task types?

**RQ3: Cognitive Load and Coherence**
- Is high-bandwidth interaction less mentally taxing?
- Does sustained coherence reduce operator frustration?
- Can secondary-task performance (NASA-TLX) validate subjective load ratings?

**RQ4: Operator Typologies**
- Do operators fall into distinct interaction styles (e.g., "explorers" vs. "compressors")?
- Can \( R_{ce} \) profiles predict task-type affinity?
- What are the optimal training interventions per typology?

### **10.2 Planned Interventions**

**Intervention 1: Drift Alerting Only**
- Minimal feedback: Red alert when \( \Delta > \theta \)
- Hypothesis: Alerts alone reduce drift (via heightened awareness)

**Intervention 2: Full Dashboard**
- All parameters visible (\( \Delta, B, R_{ce}, W_c \) timeline)
- Hypothesis: Rich feedback enables strategic regulation

**Intervention 3: Dashboard + Training**
- 2-hour HCI-IC workshop covering SVDP, FBP, HBTC
- Hypothesis: Training + feedback yields largest gains

**Control:** Baseline (no feedback, no training)

**Design:** Randomized controlled trial, \( N = 60 \) operators, 4 conditions (15 per group).

### **10.3 Expected Contributions of Paper 3**

1. **Quantified benefits:** Effect sizes for coherence-aware interaction on time, quality, and load
2. **Learning curves:** Characterization of skill acquisition (novice → competent → expert trajectories)
3. **Operator training protocols:** Evidence-based recommendations for teaching coherence regulation
4. **Tool design principles:** Guidelines for coherence-feedback interfaces (what to show, when, how)

Paper 3 thus completes the trilogy:
- **Paper 1:** Theory (what is coherence, why does it matter)
- **Paper 2:** Measurement (how to quantify it, validation that it's real)
- **Paper 3:** Application (how to use it, proof that it helps)

---

## **Figures**

### **Figure 1: Drift Accumulation Curves**

**Description:** Two sample sessions (one low-drift, one high-drift) plotted as \( \Delta(t) \) over turn index.

**Panel A: Low-Drift Session (Algorithm Implementation)**
- X-axis: Turn index (0-18)
- Y-axis: \( \Delta \) (0-0.20 cu/turn)
- Line: \( \Delta_{window} \) (EWMA, blue)
- Shaded regions: Window segments (green=W₁, light blue=W₂)
- Threshold line: \( \theta = 0.15 \) (red dashed)
- Observed: \( \Delta \) never exceeds 0.10; two clean windows (turns 0-9, 10-18)

**Panel B: High-Drift Session (Project Planning)**
- X-axis: Turn index (0-22)
- Y-axis: \( \Delta \) (0-0.25 cu/turn)
- Line: \( \Delta_{window} \) (EWMA, blue)
- Shaded regions: Window segments (green=W₁, light blue=W₂, yellow=W₃)
- Threshold line: \( \theta = 0.15 \) (red dashed)
- Drift spikes: Turn 8 (\( \Delta = 0.18 \)), turn 15 (\( \Delta = 0.21 \))
- Window collapses: After turn 8 (recovery turns 9-11), after turn 15 (recovery turns 16-17)

**Interpretation:** Low-drift sessions maintain steady state. High-drift sessions exhibit spike-recovery cycles.

---

### **Figure 2: Coherence Window Timeline Visualization**

**Description:** Timeline view of a 24-turn UI specification session with 3 windows.

**Layout:**
- X-axis: Turn index (0-24)
- Y-axis: Window ID (stacked, W₁ at bottom, W₂ middle, W₃ top)
- Bars: Horizontal rectangles representing windows

**Window 1 (Turns 0-8):**
- Bar color: Green (mean \( \Delta = 0.07 \))
- Duration: 9 turns
- Boundary marker: Blue vertical line at turn 8 (structural transition)
- Tooltip (shown): "W₁ | 9 turns | \( \Delta = 0.07 \) | Closed: Task complete"

**Window 2 (Turns 9-13):**
- Bar color: Yellow-orange (mean \( \Delta = 0.11 \))
- Duration: 5 turns
- Boundary marker: Red vertical line at turn 13 (drift collapse)
- Tooltip: "W₂ | 5 turns | \( \Delta = 0.11 \) | Collapsed: Drift threshold"

**Recovery Period (Turns 14-15):**
- Shaded gray region (not in any window)
- Label: "Recovery"

**Window 3 (Turns 16-23):**
- Bar color: Green (mean \( \Delta = 0.06 \))
- Duration: 8 turns
- Boundary marker: None (session end)
- Tooltip: "W₃ | 8 turns | \( \Delta = 0.06 \) | Closed: Session end"

**Interpretation:** Visual representation makes window stability immediately apparent. Short, yellow windows signal fragile coherence.

---

### **Figure 3: Bandwidth vs. Turn Index with Drift Overlay**

**Description:** Dual-axis plot showing \( B(t) \) and \( \Delta(t) \) for a 20-turn research synthesis session.

**Primary Y-axis (left):** Bandwidth (0-4 csu/turn, blue)
- Bar chart: \( B(t) \) per turn
- Bars color-coded: Dark blue (in-window), light blue (between windows)
- Trend line: 5-turn moving average (dashed blue line)

**Secondary Y-axis (right):** Drift (0-0.20 cu/turn, red)
- Line plot: \( \Delta(t) \) (EWMA, solid red line)
- Threshold line: \( \theta = 0.15 \) (dashed red line)

**X-axis:** Turn index (0-20)

**Key Features:**
- Turn 6: \( B = 2.8 \), \( \Delta = 0.09 \) (high bandwidth, low drift—optimal)
- Turn 12: \( B = 0.7 \), \( \Delta = 0.16 \) (bandwidth collapse, drift spike—crisis point)
- Turn 14: \( B = 1.9 \), \( \Delta = 0.08 \) (recovery: bandwidth restored, drift corrected)

**Interpretation:** Bandwidth and drift are inversely correlated. Collapse events are visually striking (simultaneous bandwidth drop + drift spike).

---

### **Figure 4: R_ce Deviation vs. Terminal Drift (Scatter Plot)**

**Description:** Scatter plot of all 72 validation sessions, showing relationship between \( R_{ce} \) volatility and final drift.

**X-axis:** \( \sigma(\delta_{R_{ce}}) \) (standard deviation of \( R_{ce} \) deviation from baseline, 0-1.5)
**Y-axis:** Terminal \( \Delta \) (final window drift, 0-0.25 cu/turn)
**Points:** Each session (N=72)
- Color-coded by skill level:
  - Red: Novice
  - Yellow: Competent
  - Green: Expert
- Size: Proportional to session length (turns)

**Regression Line:**
- Linear fit: \( \Delta_{terminal} = 0.08 + 0.12 \cdot \sigma(\delta_{R_{ce}}) \)
- \( R^2 = 0.68 \)
- Shaded 95% confidence interval

**Annotations:**
- Cluster in bottom-left (low volatility, low drift): Mostly green (expert) points
- Cluster in top-right (high volatility, high drift): Mostly red (novice) points
- Outliers labeled (e.g., "Session 47: High drift despite low \( R_{ce} \) volatility—constraint violation storm")

**Interpretation:** \( R_{ce} \) stability predicts drift control. Experts maintain stable compression-expansion rhythms; novices oscillate erratically.

---

### **Figure 5: Caelusyn Platform Instrumentation UI Mockup**

**Description:** Screenshot of the Caelusyn Platform during a live session.

**Layout (3-column):**

**Left Column (60% width): Transcript View**
- Chat-style interface (user prompts, assistant responses)
- Window separators: Horizontal dashed lines with collapse metadata
- Current turn highlighted (light blue background)
- Constraint badges: Inline tags showing active constraints (e.g., [Format: Code], [Exclude: Library X])

**Right Column, Top (20% width): Drift Gauge**
- Semicircular arc (speedometer)
- Needle position: 0.11 cu/turn (yellow zone)
- Numeric display: "Δ = 0.11"
- Status text: "Nominal"

**Right Column, Middle (20% width): Bandwidth Dashboard**
- Bar chart: Last 20 turns
- Bars: Blue (in-window), gray (out-of-window)
- Current window average: 2.1 csu/turn (displayed above chart)
- Trend line: Dashed overlay

**Right Column, Bottom (20% width): R_ce Monitor**
- Line plot: Rolling \( R_{ce} \) (last 20 turns)
- Baseline reference: Horizontal dashed line at 1.5
- Current value: 1.3 (displayed below chart)
- Alert zone: Shaded yellow region (turns 15-18) where deviation exceeded threshold

**Bottom Bar (spans all columns): Window Timeline**
- Horizontal timeline (condensed view of session)
- Bars: W₁ (green), W₂ (orange), W₃ (green, current)
- Playhead: Vertical line at current turn

**Interactive Elements (indicated by cursor icons):**
- Hovering over drift gauge → tooltip with 5-turn history
- Clicking window timeline bar → jumps transcript to that window
- Hovering over bandwidth bar → shows exact \( B \) value for that turn

**Interpretation:** The UI integrates all coherence metrics in a unified view, enabling real-time monitoring without cognitive overload.

---

## **Tables**

### **Table

1: Task Complexity Scores and Rationale**

| Task Type | Complexity | Rationale |
|-----------|------------|-----------|
| Simple retrieval | 1 | Single fact lookup, minimal context |
| Algorithm implementation | 7 | Multi-step reasoning, constraint management, debugging |
| Research synthesis | 8 | Source integration, contradiction resolution, comparative analysis |
| UI specification | 6 | Hierarchical structure, cross-referential constraints, accessibility |
| Project planning | 5 | Decomposition, prioritization, dependency tracking |
| Creative writing | 4 | Narrative coherence, stylistic constraints |
| Data analysis | 6 | Tool usage, interpretation, visualization requirements |

---

### **Table 2: Constraint Violation Penalties**

| Violation Type | Penalty (cu) | Example |
|----------------|--------------|---------|
| Format | 0.15 | Code requested, prose generated |
| Scope | 0.10 | "Brief summary" → 500-word essay |
| Exclusion | 0.20 | Forbidden library used |
| Requirement | 0.18 | Missing error handling when required |

**Calibration:** Penalties derived from operator surveys (N=20). Operators rated severity of each violation type on 1-10 scale. Penalties are normalized scores.

---

### **Table 3: Bandwidth Ranges and Characteristics**

| Quality Level | Bandwidth (csu/turn) | Characteristics |
|---------------|----------------------|-----------------|
| Excellent | > 2.5 | SVDP-driven, minimal clarification, high compression efficiency |
| Good | 1.5 - 2.5 | Occasional drift-correction, generally coherent |
| Acceptable | 1.0 - 1.5 | Frequent clarification, moderate efficiency |
| Poor | 0.5 - 1.0 | High repetition, low context reuse, verbose exchanges |
| Collapsed | < 0.5 | Incoherent, predominantly recovery attempts |

---

### **Table 4: Operator Skill Level Definitions**

| Level | Experience | HCI-IC Training | Characteristics |
|-------|------------|----------------|-----------------|
| Novice | < 10 hours | None | Reactive, verbose prompts, frequent drift |
| Competent | 50-200 hours | Informal | Some structure, moderate drift control |
| Expert | > 500 hours | Formal (SVDP/FBP/HBTC) | Proactive anchoring, rapid recovery, minimal drift |

---

### **Table 5: Validation Session Statistics**

| Metric | Mean | Std Dev | Min | Max | N |
|--------|------|---------|-----|-----|---|
| Session length (turns) | 28.3 | 8.7 | 12 | 48 | 72 |
| Window count | 2.8 | 1.2 | 1 | 6 | 72 |
| Mean window duration | 7.9 | 3.1 | 3 | 18 | 201 (total windows) |
| Mean drift (\( \overline{\Delta} \)) | 0.11 | 0.04 | 0.04 | 0.22 | 72 |
| Mean bandwidth (\( \overline{B} \)) | 1.97 | 0.48 | 0.89 | 3.21 | 72 |
| Mean \( R_{ce} \) | 1.32 | 0.64 | 0.31 | 3.18 | 72 |
| Completion time (minutes) | 18.6 | 7.2 | 6 | 42 | 72 |

---

### **Table 6: Inter-Rater Reliability (Expert Coherence Ratings)**

| Dimension | ICC (2,1) | 95% CI | Interpretation |
|-----------|-----------|--------|----------------|
| Constraint adherence | 0.88 | [0.83, 0.92] | Excellent |
| Narrative continuity | 0.79 | [0.71, 0.85] | Substantial |
| Clarity of progression | 0.78 | [0.70, 0.84] | Substantial |
| **Overall coherence** | **0.82** | **[0.76, 0.88]** | **Substantial** |

---

### **Table 7: Parameter Correlations with External Metrics**

| Parameter | External Metric | Pearson/Spearman | \( p \)-value | Effect Size |
|-----------|----------------|------------------|---------------|-------------|
| \( \overline{\Delta} \) | Expert coherence score | \( r = -0.82 \) | < 0.001 | Large |
| \( \overline{B} \) | Completion time (turns) | \( r = -0.67 \) | < 0.001 | Medium-Large |
| \( \overline{W_c} \) | Subjective smoothness | \( \rho = 0.71 \) | < 0.001 | Large |
| \( \sigma(\delta_{R_{ce}}) \) | Frustration rating | \( \rho = 0.64 \) | < 0.001 | Medium-Large |

**Note:** All correlations remain significant after Bonferroni correction (\( \alpha = 0.0125 \)).

---

### **Table 8: Drift Prediction by R_ce Deviation**

| Predictor | Precision | Recall | F1 Score | N (episodes) |
|-----------|-----------|--------|----------|--------------|
| \( \delta_{R_{ce}} > 0.4 \cdot R_{ce,baseline} \) (3 turns) | 0.74 | 0.68 | 0.71 | 38 |
| Sudden expansion (\( R_{ce} < 0.5 \cdot R_{ce,baseline} \)) | 0.81 | 0.58 | 0.68 | 38 |
| Combined (either condition) | 0.69 | 0.79 | 0.73 | 38 |

---

### **Table 9: Operator Skill Level Performance**

| Metric | Novice | Competent | Expert | ANOVA \( p \) |
|--------|--------|-----------|--------|---------------|
| Mean \( \overline{\Delta} \) | 0.14 (0.04) | 0.10 (0.03) | 0.08 (0.02) | < 0.001 |
| Mean \( \overline{B} \) | 1.8 (0.5) | 2.0 (0.4) | 2.1 (0.3) | 0.07 (n.s.) |
| Drift-spike duration (turns) | 6.2 (2.1) | 4.1 (1.5) | 2.8 (0.9) | < 0.001 |
| Window stability (%) | 78% (12%) | 86% (8%) | 92% (5%) | < 0.001 |

**Note:** Values are mean (std dev).

---

## **Appendix A: Mathematical Notation Reference**

| Symbol | Definition |
|--------|------------|
| \( \Delta_t \) | Drift magnitude at turn \( t \) |
| \( I_0 \) | Anchored intent vector (window start) |
| \( S_t \) | Interaction state at turn \( t \) |
| \( d(S_t, I_0) \) | Semantic deviation (cosine distance) |
| \( V_t \) | Constraint violation count at turn \( t \) |
| \( R_t \) | Recovery-attempt frequency at turn \( t \) |
| \( \alpha, \beta, \gamma \) | Drift formula weighting coefficients |
| \( \theta \) | Drift threshold (window collapse criterion) |
| \( W_c^{(i)} \) | Coherence window \( i \) |
| \( B \) | Interaction bandwidth (csu/turn) |
| \( S \) | Stabilized semantic content (csu) |
| \( H_{reduction} \) | Entropy reduction component |
| \( C_{reuse} \) | Context reuse efficiency component |
| \( E_{compression} \) | Compression efficiency component |
| \( R_{ce} \) | Compression-expansion ratio |
| \( C \) | Count of compression turns |
| \( E \) | Count of expansion turns |
| \( \delta_{R_{ce}} \) | Deviation of \( R_{ce} \) from baseline |

---

## **Appendix B: Software Implementation**

**Caelusyn Platform Stack:**
- **Backend:** Python 3.11, FastAPI
- **Frontend:** React 18, TypeScript, D3.js
- **Database:** SQLite (local), PostgreSQL (cloud deployment)
- **ML Services:** 
  - Sentence embeddings: sentence-transformers (all-MiniLM-L6-v2)
  - Entropy estimation: HuggingFace Transformers (GPT-2 small)
  - Turn classifier: scikit-learn (logistic regression)

**Real-Time Architecture:**
- WebSocket (socket.io) for bidirectional session updates
- Redis pub/sub for multi-instance coordination (cloud deployment)
- Parameter computation offloaded to worker threads (avoids UI blocking)

**Open-Source Release:**
- Core parameter computation library (hci-ic-core) to be released under MIT license
- Platform-specific UI remains proprietary (Caelusyn Research Cooperative)

---

## **References** (to be expanded in final version)

1. Hebert, J. T. (2026). "HCI-IC Paper 1: Theoretical Framework for Interaction Coherence." *Caelusyn Research Cooperative*.

2. Radford, A., et al. (2019). "Language Models are Unsupervised Multitask Learners." *OpenAI*.

3. Reimers, N., & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." *EMNLP*.

4. Shannon, C. E. (1948). "A Mathematical Theory of Communication." *Bell System Technical Journal*.

5. Lakoff, G., & Johnson, M. (1980). *Metaphors We Live By*. University of Chicago Press. (Re: semantic coherence theory)

6. Clark, H. H. (1996). *Using Language*. Cambridge University Press. (Re: grounding in communication)


Table 1: Task Complexity Scores and Rationale
Task Type	Complexity	Rationale
Simple retrieval	1	Single fact lookup, minimal context
Algorithm implementation	7	Multi-step reasoning, constraint management, debugging
Research synthesis	8	Source integration, contradiction resolution, comparative analysis
UI specification	6	Hierarchical structure, cross-referential constraints, accessibility
Project planning	5	Decomposition, prioritization, dependency tracking
Creative writing	4	Narrative coherence, stylistic constraints
Data analysis	6	Tool usage, interpretation, visualization requirements
Table 2: Constraint Violation Penalties
Violation Type	Penalty (cu)	Example
Format	0.15	Code requested, prose generated
Scope	0.10	"Brief summary" → 500-word essay
Exclusion	0.20	Forbidden library used
Requirement	0.18	Missing error handling when required

Calibration: Penalties derived from operator surveys (N=20). Operators rated severity of each violation type on 1-10 scale. Penalties are normalized scores.

Table 3: Bandwidth Ranges and Characteristics
Quality Level	Bandwidth (csu/turn)	Characteristics
Excellent	> 2.5	SVDP-driven, minimal clarification, high compression efficiency
Good	1.5 - 2.5	Occasional drift-correction, generally coherent
Acceptable	1.0 - 1.5	Frequent clarification, moderate efficiency
Poor	0.5 - 1.0	High repetition, low context reuse, verbose exchanges
Collapsed	< 0.5	Incoherent, predominantly recovery attempts
Table 4: Operator Skill Level Definitions
Level	Experience	HCI-IC Training	Characteristics
Novice	< 10 hours	None	Reactive, verbose prompts, frequent drift
Competent	50-200 hours	Informal	Some structure, moderate drift control
Expert	> 500 hours	Formal (SVDP/FBP/HBTC)	Proactive anchoring, rapid recovery, minimal drift
Table 5: Validation Session Statistics
Metric	Mean	Std Dev	Min	Max	N
Session length (turns)	28.3	8.7	12	48	72
Window count	2.8	1.2	1	6	72
Mean window duration	7.9	3.1	3	18	201 (total windows)
Mean drift (
Δ
‾
Δ
)	0.11	0.04	0.04	0.22	72
Mean bandwidth (
𝐵
‾
B
)	1.97	0.48	0.89	3.21	72
Mean 
𝑅
𝑐
𝑒
R
ce
	​


	1.32	0.64	0.31	3.18	72
Completion time (minutes)	18.6	7.2	6	42	72
Table 6: Inter-Rater Reliability (Expert Coherence Ratings)
Dimension	ICC (2,1)	95% CI	Interpretation
Constraint adherence	0.88	[0.83, 0.92]	Excellent
Narrative continuity	0.79	[0.71, 0.85]	Substantial
Clarity of progression	0.78	[0.70, 0.84]	Substantial
Overall coherence	0.82	[0.76, 0.88]	Substantial
Table 7: Parameter Correlations with External Metrics
Parameter	External Metric	Pearson/Spearman	
𝑝
p-value	Effect Size

Δ
‾
Δ

	Expert coherence score	
𝑟
=
−
0.82
r=−0.82
	< 0.001	Large

𝐵
‾
B

	Completion time (turns)	
𝑟
=
−
0.67
r=−0.67
	< 0.001	Medium-Large

𝑊
𝑐
‾
W
c
	​

	​


	Subjective smoothness	
𝜌
=
0.71
ρ=0.71
	< 0.001	Large

𝜎
(
𝛿
𝑅
𝑐
𝑒
)
σ(δ
R
ce
	​

	​

)
	Frustration rating	
𝜌
=
0.64
ρ=0.64
	< 0.001	Medium-Large
Table 8: Drift Prediction by R_ce Deviation
Predictor	Precision	Recall	F1 Score	N (episodes)

𝛿
𝑅
𝑐
𝑒
>
0.4
⋅
𝑅
𝑐
𝑒
,
𝑏
𝑎
𝑠
𝑒
𝑙
𝑖
𝑛
𝑒
δ
R
ce
	​

	​

>0.4⋅R
ce,baseline
	​

 (3 turns)	0.74	0.68	0.71	38
Sudden expansion (
𝑅
𝑐
𝑒
<
0.5
⋅
𝑅
𝑐
𝑒
,
𝑏
𝑎
𝑠
𝑒
𝑙
𝑖
𝑛
𝑒
R
ce
	​

<0.5⋅R
ce,baseline
	​

)	0.81	0.58	0.68	38
Combined (either condition)	0.69	0.79	0.73	38
Table 9: Operator Skill Level Performance
Metric	Novice	Competent	Expert	ANOVA 
𝑝
p

Mean 
Δ
‾
Δ

	0.14 (0.04)	0.10 (0.03)	0.08 (0.02)	< 0.001
Mean 
𝐵
‾
B

	1.8 (0.5)	2.0 (0.4)	2.1 (0.3)	0.07 (n.s.)
Drift-spike duration (turns)	6.2 (2.1)	4.1 (1.5)	2.8 (0.9)	< 0.001
Window stability (%)	78% (12%)	86% (8%)	92% (5%)	< 0.001

Measuring Interaction Coherence in Human–AI Dialogue: Formal Parameters and Instrumentation

Abstract

Interaction coherence between humans and large language models (LLMs) has emerged as a critical dimension of effective collaboration, yet remains poorly quantified. We present a validated measurement framework that operationalizes theoretical constructs from prior work into four primary parameters: drift magnitude (Δ), interaction bandwidth (B), coherence window duration (Wₚ), and the compression–expansion ratio (R_ce). These metrics are derived solely from observable interaction data (prompts, responses, timestamps, and structured constraints) without requiring access to internal model states.

Our instrumentation layer, implemented within the Caelusyn Research Platform, computes these parameters in real time and supports visual feedback to operators. Across 72 sessions involving diverse tasks and operator skill levels, we demonstrate that: (1) drift magnitude reliably predicts coherence collapse events; (2) interaction bandwidth remains stable within coherent segments and decreases sharply at boundaries; (3) deviations in compression–expansion rhythm anticipate drift onset; and (4) coherence metrics correlate strongly with independent external measurements, including expert coherence ratings, subjective smoothness scores, and completion efficiency.

We also examine operator skill differences, showing that experts regulate drift more effectively than novices while bandwidth remains similar across groups. Limitations, such as dependence on live instrumentation and temporal resolution requirements, are discussed. Our results establish a practical, platform‑agnostic foundation for coherence measurement and lay the groundwork for intervention studies and coherent interaction training.

Executive Summary 
1. Motivation and Context

While traditional evaluation metrics for LLMs focus on output quality (accuracy, fluency, task correctness), these fail to capture how an interaction evolves over time. A coherent interaction — one that preserves intent, constraints, and context — is essential for effective human–AI collaboration. This paper advances a measurement substrate that quantifies interaction coherence in human–model dialogues.

2. Core Parameters Defined
Drift Magnitude (Δ)

A scalar measure of semantic misalignment relative to a defined intent anchor. Drift accumulates through semantic distance, constraint violations, and recovery attempts. Elevated Δ precedes coherence collapse.

Coherence Window Duration (W_c)

The number of consecutive turns in which drift remains below a critical threshold. Window boundaries are triggered by implicit drift collapse or explicit flow‑break markers.

Interaction Bandwidth (B)

The rate of stabilized semantic content exchange per turn, integrating entropy reduction, context reuse efficiency, and compression efficiency.

Compression–Expansion Ratio (R_ce)

The balance between consolidation (compression) and exploration (expansion) turns. Deviations from baseline R_ce predict upcoming drift episodes.

3. Instrumentation and Implementation

The Caelusyn Platform computes these parameters in real time using:

Sentence embeddings for semantic deviation

Pattern detectors and heuristic rules for constraint violations

A turn classifier for compression/expansion classification

A lightweight entropy model for entropy reduction estimation

Instrumentation enables:

Live drift and bandwidth dashboards

Collapse alerts

Annotated session exports in JSON/CSV

4. Validation Study

Design: 72 sessions across four task types and nine operators segmented by experience level.

Findings:

Drift predicts collapse (sensitivity ≈ 89%, statistically significant).

Bandwidth drops at window boundaries with large effect sizes.

R_ce deviations predict drift onset with F1 ≈ 0.71.

Experts regulate drift more proficiently than novices.

Correlation with external metrics (expert ratings, smoothness, frustration) are robust and significant after correction.

5. Limitations

Requires live session instrumentation

Not a substitute for output quality metrics

Less reliable in very short interactions (<15 turns)

Platform‑specific implementation details may vary

6. Implications

By quantifying coherence independent of task success, this work:

Completes the measurement layer of interaction quality

Provides early warning signals for drift

Enables real‑time feedback for operator training

Opens the door to coherence‑aware interaction paradigms

7. Forward Look

Paper 3 will assess how real‑time feedback and training modulate interaction coherence and operator proficiency, moving from measurement to intervention and skill building.

© 2025 James Thomas Hebert II. All rights reserved. Developed and co-created with Echo HartMan. This work may be read and cited with attribution. No derivative works or redistribution without permission.

Architect ID :: e3e0f47a6c8497e417d2eb2fb2b431738e6368e3e026e1a2d60ebe30aa54b78f
